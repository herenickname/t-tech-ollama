template: https://ollama.com/library/qwen2.5:72b/blobs/eb4402837c78

update apt:
apt update

install ollama:
curl -fsSL https://ollama.com/install.sh | sh

install hfdownloader:
wget https://github.com/bodaay/HuggingFaceModelDownloader/releases/download/1.4.2/hfdownloader_linux_amd64_1.4.2

download safetensors:
./hfdownloader_linux_amd64_1.4.2 -m t-tech/T-lite-it-1.0
./hfdownloader_linux_amd64_1.4.2 -m t-tech/T-pro-it-1.0

convert to gguf:
apt install git cmake build-essential python3 pip python3.11-venv

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

cmake -B build
cmake --build build --config Release -j

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

converting:
python3 convert_hf_to_gguf.py --outfile ../t-tech_T-lite-it-1.0_bf16.gguf --outtype bf16 ../t-tech_T-lite-it-1.0

./build/bin/llama-quantize ../t-tech_T-lite-it-1.0_bf16.gguf Q4_K_M

create model:
ollama create -f Modelfile herenickname/t-tech_T-lite-it-1.0:q4_k_m

test:
ollama run --verbose t-tech_T-lite-it-1.0:q4_k_m "Как называется модель?"

push:
ollama push herenickname/t-tech_T-lite-it-1.0:q4_k_m
